{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhP4Eu4BN77BUUZvVNDbm0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatemehAbediK/signal-processing-EEG/blob/main/Teaching_signal_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##mne.tools/mne study template , can be a help!!\n",
        "\n",
        "##Anaconda python----MNE environment---jupyter lab\n",
        "\n",
        "## conda activate mne\n",
        "## conda install jupyter lab"
      ],
      "metadata": {
        "id": "pKRxIxaUOE3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import pathlib\n",
        "import mne\n",
        "###Ensure Matplotlib uses the Qt5Agg backend,\n",
        "## which is the best choice for MNE-Python's interactive plotting functions.\n",
        "matplotlib.use('Qt5Agg')\n"
      ],
      "metadata": {
        "id": "YODiB6IuOTeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Retrieve the storage location of the sample data, and download the dataset if it cannot be found.\n",
        "sample_data_dir = mne.datasets.sample.data_path()\n",
        "# Convert to a pathlib.Path for more convenience\n",
        "sample_data_dir = pathlib.Path(sample_data_dir)\n",
        "sample_data_dir\n"
      ],
      "metadata": {
        "id": "91zOQUhyOSZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Load some raw data!\n",
        "raw_path = sample_data_dir / '.....fif'\n",
        "raw = mne.io.read_raw(raw_path)\n",
        "raw\n",
        "\n",
        "raw.plot()\n",
        "#we go to the new window of the EEg so like the eeg lab it is great & it's interactive!!"
      ],
      "metadata": {
        "id": "TE5vdAVpOWQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Extract events from the channels\n",
        "events = mne.find_events(raw)\n",
        "event_id = {\n",
        "    'Auditory/Left': 1,\n",
        "    'Auditory/Right': 2,\n",
        "    'Visual/Left': 3,\n",
        "    'Visual/Right': 4,\n",
        "    'Smiley': 5,\n",
        "    'Button': 32\n",
        "}\n",
        "event_id"
      ],
      "metadata": {
        "id": "D7rG7t9YOdsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Plot the raw data again, but add event markers----it will addd the vertical lines with the names of event-ids\n",
        "raw.plot(events=events, event_id=event_id)\n"
      ],
      "metadata": {
        "id": "VGdZyBfbOgey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw.info\n",
        "raw.info['meas_date']\n",
        "raw.info['sfreq']\n",
        "raw.info['bads']\n",
        "raw.ch_names\n"
      ],
      "metadata": {
        "id": "1kfWAOLKOkfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example:\n",
        "\n",
        "raw.ch_names[:10]\n",
        "\n",
        "raw.info['chs'][0]\n"
      ],
      "metadata": {
        "id": "7EiAEMHzOleS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Visualize the sensor locations\n",
        "raw.plot_sensors(ch_type='eeg')\n",
        "raw.plot_sensors(kind='3d', ch_type='eeg')"
      ],
      "metadata": {
        "id": "HHFSqSaROpul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Mark channels as bad\n",
        "#Mark an additional EEG channel as bad and view the topoplot.\n",
        "\n",
        "raw.info['bads']\n",
        "raw.info['bads'] += ['EEG 051']\n",
        "\n",
        "raw.plot_sensors(ch_type='eeg')"
      ],
      "metadata": {
        "id": "WkQ-QsyKOpsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Select only a subset of the channels(eeg & eog)----but first we make a copy then pick our certain types\n",
        "raw_eeg = raw.copy().pick_types(meg=False, eeg=True, eog=True, exclude=[])      #exclude :do not delete anything !give us all even the bad channels!\n",
        "len(raw_eeg.ch_names)\n",
        "\n",
        "#check again!\n",
        "raw_eeg.info\n",
        "raw_eeg.plot(events=events, event_id=event_id)"
      ],
      "metadata": {
        "id": "wReirogdOpqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Crop and filter the data\n",
        "raw_eeg_cropped = raw_eeg.copy().crop(tmax=100)\n",
        "raw_eeg_cropped.times[-1]\n",
        "\n",
        "raw_eeg_cropped.load_data()\n",
        "raw_eeg_cropped_filtered = raw_eeg_cropped.copy().filter(l_freq=0.1, h_freq=40)\n",
        "\n",
        "##for comparison:\n",
        "raw_eeg_cropped.plot(events=events, event_id=event_id)\n",
        "raw_eeg_cropped_filtered.plot(events=events, event_id=event_id)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(2)\n",
        "\n",
        "raw_eeg_cropped.plot_psd(ax=ax[0], show=False)\n",
        "raw_eeg_cropped_filtered.plot_psd(ax=ax[1], show=False)\n",
        "\n",
        "ax[0].set_title('PSD before filtering')\n",
        "ax[1].set_title('PSD after filtering')\n",
        "ax[1].set_xlabel('Frequency (Hz)')\n",
        "fig.set_tight_layout(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nzx5kHFEOzxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##Save the data\n",
        "raw_eeg_cropped_filtered.save(pathlib.Path('out_data') / 'eeg_cropped_filt_raw.fif',\n",
        "                              overwrite=True)"
      ],
      "metadata": {
        "id": "fUH01EXSOzu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#BIDS:brain image data structure"
      ],
      "metadata": {
        "id": "VIR6K3pPO8z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://mne-tools/mne-bids\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import pathlib\n",
        "\n",
        "import mne\n",
        "import mne_bids\n",
        "\n",
        "matplotlib.use('Qt5Agg')\n"
      ],
      "metadata": {
        "id": "DzyUhzJ1PA47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#raw data\n",
        "sample_data_dir = mne.datasets.sample.data_path()\n",
        "sample_data_dir = pathlib.Path(sample_data_dir)\n",
        "\n",
        "raw_path = sample_data_dir / '....fif'\n",
        "raw = mne.io.read_raw(raw_path)\n",
        "\n",
        "events = mne.find_events(raw)\n",
        "event_id = {\n",
        "    'Auditory/Left': 1,\n",
        "    'Auditory/Right': 2,\n",
        "    'Visual/Left': 3,\n",
        "    'Visual/Right': 4,\n",
        "    'Smiley': 5,\n",
        "    'Button': 32\n",
        "}\n",
        "\n",
        "raw.info()"
      ],
      "metadata": {
        "id": "-6pJmZTXPJH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we can addd sth beased on the mne.tools website instructions:\n",
        "\n",
        "subject_info={\n",
        "    'birthday' : (1998 , 10 , 1),\n",
        "    'sex' : 2       #2 is for female!\n",
        "    'hand' : 3      #3 for abmivent hand(both right and left handed)\n",
        "}\n",
        "\n",
        "raw.info['subject_info']=subject_info\n",
        "\n",
        "#let's check\n",
        "raw.info                   #it should be added"
      ],
      "metadata": {
        "id": "mTo2gKntPMcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Write the raw data to BIDS!!\n",
        "\n",
        "raw.info['line_freq'] = 60          ##based on the lab which you work in!\n",
        "\n",
        "out_path = pathlib.Path('out_data/sample_BIDS')\n",
        "bids_path = mne_bids.BIDSPath(subject='01',            #these are the things in the BIDS files!!\n",
        "                              session='01',\n",
        "                              task='audiovisual',\n",
        "                              run='01',\n",
        "                              root=out_path)\n",
        "\n",
        "mne_bids.write_raw_bids(raw, bids_path=bids_path, events_data=events,\n",
        "                        event_id=event_id, overwrite=True , annonymize=None)\n",
        " #annonomyzie is for shifting the time the data was recorded and to delete some of the meta data whcih you don't need"
      ],
      "metadata": {
        "id": "pH9Yk1sePbmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##having a summary!!\n",
        "mne_bids.print_dir_tree(out_path)\n",
        "print(mne_bids.make_report(out_path))"
      ],
      "metadata": {
        "id": "n3unGI6bPbha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Reading BIDS data\n",
        "bids_root = pathlib.Path('out_data/sample_BIDS')\n",
        "\n",
        "bids_path = mne_bids.BIDSPath(subject='01',\n",
        "                              session='01',\n",
        "                              task='audiovisual',\n",
        "                              run='01',\n",
        "                              datatype='meg',\n",
        "                              root=bids_root)\n",
        "\n",
        "raw = mne_bids.read_raw_bids(bids_path)\n",
        "\n",
        "raw.plot()\n",
        "\n",
        "\n",
        "#mne.bids will add the event it finds as an annotations to the raw file!!"
      ],
      "metadata": {
        "id": "wuFPK6Z1Pd6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####Events are stored as annotations – but we convert between the two.\n",
        "\n",
        "raw.annotations[0]\n",
        "events, event_id = mne.events_from_annotations(raw)\n",
        "\n",
        "mne.viz.plot_events(events, event_id=event_id, sfreq=raw.info['sfreq'])\n",
        "\n",
        "\n",
        "#finding the paths:\n",
        "\n",
        "bids_path.meg_calibration_fpath\n",
        "bids_path.meg_crosstalk_fpath"
      ],
      "metadata": {
        "id": "OfQZGci7Po6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating epochs and generating evoked responses (ERP/ERF)\n",
        "\n",
        "bids_root = pathlib.Path('out_data/sample_BIDS')\n",
        "\n",
        "bids_path = mne_bids.BIDSPath(subject='01',\n",
        "                              session='01',\n",
        "                              task='audiovisual',\n",
        "                              run='01',\n",
        "                              datatype='meg',\n",
        "                              root=bids_root)\n",
        "\n",
        "raw = mne_bids.read_raw_bids(bids_path)\n",
        "raw.load_data()\n",
        "\n",
        "\n",
        "raw.filter(l_freq=0.1, h_freq=40)\n",
        "events, event_id = mne.events_from_annotations(raw)\n",
        "\n",
        "\n",
        "tmin = -0.3\n",
        "tmax = 0.5\n",
        "baseline = (None, 0)          #None means from the beginning of the epoch    0 means till the event\n",
        "\n",
        "epochs = mne.Epochs(raw,\n",
        "                    events=events,\n",
        "                    event_id=event_id,\n",
        "                    tmin=tmin,\n",
        "                    tmax=tmax,\n",
        "                    baseline=baseline,       #if you say the baseline to be None here it won't do the baseline correction nut we wants it here!\n",
        "                    preload=True)            #to load the data to the memory\n",
        "epochs\n",
        "\n",
        "epochs.plot()\n"
      ],
      "metadata": {
        "id": "0aGYT25iPqAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we can select  epochs based on experimental conditions\n",
        "epochs['Auditory/Right']\n",
        "epochs['Auditory']\n",
        "epochs['Left']\n",
        "epochs['Visual'].plot_image()\n",
        "\n",
        "##let's selcet the right epochs from the eeg only:\n",
        "epochs['Right'].copy().pick_types(meg=False , eog=False ,eeg=True).plot_image()\n",
        "\n",
        "#or\n",
        "epochs['Right'].plot_image(picks=['EEG'])\n",
        "\n",
        "#Saving epochs\n",
        "epochs.save(pathlib.Path('out_data') / 'epochs_epo.fif',overwrite=True)          #for epochs use \"\"\"_epo\"\"\" !"
      ],
      "metadata": {
        "id": "0r1TnxPpP68C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Creating evoked data=-----average of epocks !\n",
        "\n",
        "evoked_auditory = epochs['Auditory'].average()\n",
        "evoked_visual = epochs['Visual'].average()\n",
        "\n",
        "#VISUALIZATION!!\n",
        "evoked_auditory.plot(spatial_colors=True)\n",
        "evoked_auditory.plot_topomap(ch_type='mag' , times=[0,0.1 , 0.2 , 0.3 , 0.4 ,0.5])\n",
        "evoked_auditory.plot_joint(picks='mag')\n",
        "mne.viz.plot_compare_evokeds([evoked_auditory, evoked_visual], picks='mag')\n",
        "\n",
        "##Saving evoked data\n",
        "mne.write_evokeds(fname=pathlib.Path('out_data') / 'evokeds_ave.fif',\n",
        "                  evoked=[evoked_auditory, evoked_visual])\n",
        "\n",
        "\n",
        "##Reading evoked data\n",
        "#ALL:\n",
        "evokeds = mne.read_evokeds(fname=pathlib.Path('out_data') / 'evokeds_ave.fif')\n",
        "\n",
        "evokeds\n",
        "\n",
        "#JUST ONE :\n",
        "evokeds[0]\n",
        "evoked = mne.read_evokeds(fname=pathlib.Path('out_data') / 'evokeds_ave.fif',\n",
        "                          condition='0.50 * Visual/Left + 0.50 * Visual/Right')\n",
        "evoked"
      ],
      "metadata": {
        "id": "QI-cX_SqP7rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CLEANING----DELETING ARTIFACTS"
      ],
      "metadata": {
        "id": "tzUgFOyEQHcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs.drop_bad\n",
        "\n",
        "epochs = mne.read_epochs(pathlib.Path('out_data') / 'epochs_epo.fif')\n",
        "epochs\n",
        "\n",
        "epochs.apply_baseline((None, 0))\n",
        "epochs.plot()\n",
        "\n",
        "\n",
        "#Reject artifacts based on channel signal amplitude:\n",
        "\n",
        "reject_criteria = dict(mag=3000e-15,     # 3000 fT\n",
        "                       grad=3000e-13,    # 3000 fT/cm\n",
        "                       eeg=150e-6,       # 150 µV\n",
        "                       eog=200e-6)       # 200 µV\n",
        "\n",
        "flat_criteria = dict(mag=1e-15,          # 1 fT\n",
        "                     grad=1e-13,         # 1 fT/cm\n",
        "                     eeg=1e-6)           # 1 µV\n",
        "\n",
        "\n",
        "epochs.drop_bad(reject=reject_criteria, flat=flat_criteria)\n",
        "epochs.plot_drop_log()\n",
        "\n",
        "epochs['Visual'].plot_image()\n",
        "epochs.plot_sensors(ch_type='eeg')\n",
        "epochs['Visual'].plot_image(picks='EEG 060')"
      ],
      "metadata": {
        "id": "JShr4SUdQQxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1)\n",
        "####SSP:signal space projection\n",
        "#Let's see if we can retain most the epochs, but still get rid of the EOG artifact. And while we're at it, the ECG artifacts too 😉\n",
        "bids_root = pathlib.Path('out_data/sample_BIDS')\n",
        "\n",
        "bids_path = mne_bids.BIDSPath(subject='01',\n",
        "                              session='01',\n",
        "                              task='audiovisual',\n",
        "                              run='01',\n",
        "                              datatype='meg',\n",
        "                              root=bids_root)\n",
        "\n",
        "raw = mne_bids.read_raw_bids(bids_path)\n",
        "raw.load_data()\n",
        "\n",
        "raw.filter(l_freq=0.1, h_freq=40)\n",
        "\n",
        "ecg_projs, ecg_events = mne.preprocessing.compute_proj_ecg(raw, n_grad=1, n_mag=1, n_eeg=0,             #ecg artifacts\n",
        "                                                           average=True)\n",
        "\n",
        "eog_projs, eog_events = mne.preprocessing.compute_proj_eog(raw, n_grad=1, n_mag=1,n_eeg=1,             #eog artifacts\n",
        "                                                           average=True)\n",
        "\n",
        "ecg_proj\n",
        "eog_projs\n",
        "\n",
        "projs = eog_projs + ecg_projs\n",
        "projs\n",
        "\n",
        "epochs.add_proj(projs)\n",
        "epochs.plot()\n",
        "\n",
        "\n",
        "epochs_cleaned = epochs.copy().apply_proj()\n",
        "\n",
        "epochs_cleaned['Visual'].plot_image()\n",
        "epochs_cleaned['Visual'].plot_image(picks='EEG 060')\n",
        "\n",
        "#2)\n",
        "####ICA\n",
        "##for removing eog (eye blinks)  & ecg (hearbeats)\n",
        "#First, start with the raw data again and apply a 1.0 Hz high-pass filter, which is advantegeous for ICA performance.\n",
        "\n",
        "bids_root = pathlib.Path('out_data/sample_BIDS')\n",
        "\n",
        "bids_path = mne_bids.BIDSPath(subject='01',\n",
        "                              session='01',\n",
        "                              task='audiovisual',\n",
        "                              run='01',\n",
        "                              datatype='meg',\n",
        "                              root=bids_root)\n",
        "\n",
        "raw = mne_bids.read_raw_bids(bids_path)\n",
        "raw.load_data()\n",
        "raw.filter(l_freq=1, h_freq=40)  # High-pass with 1. Hz cut-off is recommended for ICA\n",
        "\n",
        "epochs = mne.read_epochs(pathlib.Path('out_data') / 'epochs_epo.fif')\n",
        "epochs_selection = epochs.selection\n",
        "epochs_selection\n",
        "\n",
        "epochs.plot()--------#we cn delete some data or ba channels here\n",
        "\n",
        "epochs_selection    --------------#do it again &  will see some epochs are gone!!\n",
        "\n",
        "#Only keep the subset of events that corresponds to the retained epochs.\n",
        "events, event_id = mne.events_from_annotations(raw)\n",
        "events = events[epochs_selection]\n",
        "\n",
        "events.shape()\n",
        "\n",
        "\n",
        "#Create epochs for ICA. All parameters should match exactly the ones of the epochs we intend to clean.\n",
        "\n",
        "tmin = epochs.tmin\n",
        "tmax = epochs.tmax\n",
        "baseline = (None, 0)\n",
        "\n",
        "epochs_ica = mne.Epochs(raw,\n",
        "                        events=events,\n",
        "                        event_id=event_id,\n",
        "                        tmin=tmin,\n",
        "                        tmax=tmax,\n",
        "                        baseline=baseline,\n",
        "                        preload=True)\n",
        "\n",
        "\n",
        "\n",
        "#Finally, fit ICA!\n",
        "\n",
        "epochs_ica.info\n",
        "\n",
        "\n",
        "n_components = 0.8                     # Should normally be higher, like 0.999!!\n",
        "method = 'picard'                      #3 methods 3 algorithms : fast ICA , info max ICA ,picard ICA\n",
        "max_iter = 100                         # Should normally be higher, like 500 or even 1000!!\n",
        "fit_params = dict(fastica_it=5)\n",
        "random_state = 42\n",
        "\n",
        "ica = mne.preprocessing.ICA(n_components=n_components,\n",
        "                            max_pca_components=300,\n",
        "                            method=method,\n",
        "                            max_iter=max_iter,\n",
        "                            fit_params=fit_params,\n",
        "                            random_state=random_state)\n",
        "\n",
        "\n",
        "ica.fit(epochs_ica)\n",
        "\n",
        "ica.plot_components(inst=epochs)\n",
        "\n",
        "\n",
        "##Detect ECG and EOG patterns\n",
        "ecg_epochs = mne.preprocessing.create_ecg_epochs(raw, reject=None,\n",
        "                                                 baseline=(None, -0.2),           #starts at the first of the epoch and ends 0.2 seconds before the end of the heart beat\n",
        "                                                 tmin=-0.5, tmax=0.5)\n",
        "ecg_evoked = ecg_epochs.average()\n",
        "ecg_inds, ecg_scores = ica.find_bads_ecg(ecg_epochs, method='ctps')\n",
        "\n",
        "\n",
        "eog_epochs = mne.preprocessing.create_eog_epochs(raw, reject=None,\n",
        "                                                 baseline=(None, -0.2),\n",
        "                                                 tmin=-0.5, tmax=0.5)\n",
        "eog_evoked = eog_epochs.average()\n",
        "eog_inds, eog_scores = ica.find_bads_eog(eog_epochs , method='ctps')\n",
        "\n",
        "\n",
        "components_to_exclude = ecg_inds + eog_inds\n",
        "ica.exclude = components_to_exclude\n",
        "\n",
        "ica.exclude\n",
        "\n",
        "#Plot automated artifact detection scores------when ICA is done it gives scores for the bad channels it finds!!!\n",
        "ica.plot_scores(ecg_scores)\n",
        "ica.plot_scores(eog_scores)\n",
        "\n",
        "#Plot ICA sources\n",
        "ica.plot_sources(ecg_evoked)\n",
        "\n",
        "\n",
        "#Plot overlay of original and cleaned data\n",
        "ica.plot_overlay(ecg_evoked)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###NOW CLEAN THE DATA(DELETE THOSE BAD CHANNELS IVA FOUND!)\n",
        "\n",
        "epoches_cleaned=ica.apply(epochs.copy())\n",
        "\n",
        "#for comparison:\n",
        "epoches_cleaned.plot(title='after ICA')\n",
        "epoches.plot(title='before ICA')\n",
        "\n",
        "\n",
        "epoches_cleaned.save('cleaned epoches after ICA')"
      ],
      "metadata": {
        "id": "3axDadmgQvVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##WELL DONE TILL HERE !!"
      ],
      "metadata": {
        "id": "KovXer6vQ0Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###NOW :\n",
        "#Let's Load EEGLAB epochs and setting the proper montage (electrode locations) for EEG data\n",
        "\n",
        "epochs_fname = pathlib.Path('eeg_montage_data') / 'sample_eeglab_epochs.set'\n",
        "epochs = mne.read_epochs_eeglab(epochs_fname)\n",
        "\n",
        "epochs\n",
        "\n",
        "epochs.plot_sensors()\n",
        "\n",
        "#Load vendor-supplied montage:where were the sensors on the persons head!!\n",
        "\n",
        "montage_fname = pathlib.Path('eeg_montage_data') / 'achtiCHamp_64_channels_and_fiducials_Theta_Phi.txt'\n",
        "dig_montage = mne.channels.read_custom_montage(montage_fname)\n",
        "dig_montage.plot(sphere='auto')\n",
        "\n",
        "#Apply this montage to the loaded epochs\n",
        "epochs.set_montage(dig_montage)\n",
        "\n",
        "epochs.plot_sensors()\n"
      ],
      "metadata": {
        "id": "oZXH2oS9RDpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANALYSIS"
      ],
      "metadata": {
        "id": "vj8TFcRfRMBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##NOW ANALYSIS!!!!!\n",
        "\n",
        "#Multivariate Statistics (Decoding / MVPA) on MEG/EEG Data\n",
        "%matplotlib qt\n",
        "import pathlib\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import mne\n",
        "\n",
        "matplotlib.use('Qt5Agg')\n",
        "mne.set_log_level('warning')\n",
        "\n",
        "\n",
        "epochs = mne.read_epochs(pathlib.Path('out_data') / 'epochs_epo.fif')\n",
        "epochs.apply_baseline((None, 0))\n",
        "\n",
        "epochs\n",
        "\n",
        "\n",
        "##Select epochs of interest.Here, we intend to analyze the auditory epochs.\n",
        "\n",
        "epochs_auditory = epochs['Auditory']\n",
        "\n",
        "epochs_auditory\n",
        "\n",
        "#Calculate empirical evoked difference\n",
        "\n",
        "evoked_diff = mne.combine_evoked(\n",
        "    [epochs_auditory['Auditory/Left'].average(),\n",
        "     epochs_auditory['Auditory/Right'].average()],\n",
        "     weights=[1, -1]  # Subtraction\n",
        ")\n",
        "\n",
        "evoked_diff.plot(gfp=True)\n",
        "\n",
        "mne.viz.plot_compare_evokeds(\n",
        "    [epochs_auditory['Auditory/Left'].average(),\n",
        "     epochs_auditory['Auditory/Right'].average(),\n",
        "     evoked_diff]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "6wVGyAczRPvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOME ML!!!!"
      ],
      "metadata": {
        "id": "Z50d23caRtGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Okay, but… we want more than that! Let's do some machine learning!!!!\n",
        "\n",
        "#Equalize the number of epochs\n",
        "#To keep chance level at 50% accuracy, we first equalize the number of epochs in each condition.\n",
        "\n",
        "epochs_auditory.equalize_event_counts(epochs_auditory.event_id)\n",
        "\n",
        "epochs_auditory\n",
        "\n",
        "\n",
        "#Create input X and response y.\n",
        "#A classifier takes as input a matrix X and returns a vector y (consisting of 0 and 1).\n",
        "#Here X will be the data at one time point on all gradiometers (hence the term multivariate).\n",
        "#We want to train our model to discriminate between the Auditory/Left and the Auditory/Right trials.\n",
        "#We work with all sensors jointly and try to find a discriminative pattern between the two conditions to predict the experimental condition of individual trials.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Create an vector with length = no. of trials.\n",
        "y = np.empty(len(epochs_auditory.events), dtype=int)\n",
        "\n",
        "# Which trials are LEFT, which are RIGHT?\n",
        "idx_left = epochs_auditory.events[:, 2] == epochs_auditory.event_id['Auditory/Left']\n",
        "idx_right = epochs_auditory.events[:, 2] == epochs_auditory.event_id['Auditory/Right']\n",
        "\n",
        "#we'll have a list of TRUE & FALSEs\n",
        "idx_left\n",
        "idx_right\n",
        "\n",
        "\n",
        "# Encode: LEFT = 0, RIGHT = 1.\n",
        "y[idx_left] = 0\n",
        "y[idx_right] = 1\n",
        "\n",
        "print(y)\n",
        "print(f'\\nSize of y: {y.size}')\n",
        "\n",
        "\n",
        "#Now, let's create the input matrix, X.\n",
        "\n",
        "#We wish to focus only on the gradiometer channels here, so we use pick_types(meg='grad').\n",
        "#For magnetometer channels, we would need to pass meg='mag'; and for EEG channels: meg=False, eeg=True.\n",
        "#We create a copy of the epochs because pick_types() operates in-place, but we would like to keep the original epochs object untouched.\n",
        "\n",
        "\n",
        "epochs_auditory_grad = epochs_auditory.copy().pick_types(meg='grad')\n",
        "\n",
        "\n",
        "data = epochs_auditory_grad.get_data()         # The array has the shape: (n_trials, n_channels, n_timepoints)\n",
        "print(data.shape)\n",
        "\n",
        "#We need to reshape the array such that for each trial, we have a vector [channel_1_time_1, channel_1_time_2, ..., channel_m_time_n], i.e.,\n",
        "#we aim to reshape X to the dimension (n_trials, n_channels * n_timepoints).\n",
        "\n",
        "n_trials = data.shape[0]\n",
        "\n",
        "X = data.reshape(n_trials, -1)\n",
        "print(X)\n",
        "print(X.shape)\n"
      ],
      "metadata": {
        "id": "qM_diWujRsyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#####NOW:\n",
        "#Create a classifier!\n",
        "#We will use plain scikit-learn machinery for the first round of classifications.\n",
        "#This is to demonstrate that you can simply feed pre-processed data from MNE into scikit-learn.\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "# The classifier pipeline: it is extremely important to scale the data\n",
        "# before running the actual classifier (logistic regression in our case).\n",
        "clf = make_pipeline(StandardScaler(),LogisticRegression())\n",
        "\n",
        "# Run cross-validation.\n",
        "# CV without shuffling – \"block cross-validation\" – is what we want here\n",
        "\n",
        "n_splits = 5\n",
        "scoring = 'roc_auc'\n",
        "cv = StratifiedKFold(n_splits=n_splits)\n",
        "scores = cross_val_score(clf, X=X, y=y, cv=cv, scoring=scoring)\n",
        "\n",
        "# Mean and standard deviation of ROC AUC across cross-validation runs.\n",
        "roc_auc_mean = round(np.mean(scores), 3)\n",
        "roc_auc_std = round(np.std(scores), 3)\n",
        "\n",
        "print(f'CV scores: {scores}')\n",
        "print(f'Mean ROC AUC = {roc_auc_mean:.3f} (SD = {roc_auc_std:.3f})')\n",
        "\n",
        "\n",
        "#Visualize the cross-validation results.\n",
        "fig, ax = plt.subplots()\n",
        "ax.boxplot(scores,\n",
        "           showmeans=True, # Green triangle marks the mean.\n",
        "           whis=(0, 100),  # Whiskers span the entire range of the data.\n",
        "           labels=['Left vs Right'])\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Cross-Validation Scores')\n",
        "\n",
        "\n",
        "#####We can do this more simply using the mne.decoding module! Let's go. 🚀\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from mne.decoding import Scaler, Vectorizer, cross_val_multiscore\n",
        "\n",
        "\n",
        "# First, create X and y.\n",
        "epochs_auditory_grad = epochs_auditory.copy().pick_types(meg='grad')\n",
        "X = epochs_auditory_grad.get_data()\n",
        "y = epochs_auditory_grad.events[:, 2]\n",
        "\n",
        "# Classifier pipeline.\n",
        "clf = make_pipeline(\n",
        "                                       # An MNE scaler that correctly handles different channel types –\n",
        "   Scaler(epochs_grad.info),\n",
        "                                        # Remember this annoying and error-prone NumPy array reshaping we had to dO earlier? Not anymore, thanks to the MNE vectorizer!\n",
        "    Vectorizer(),\n",
        "                                         # And, finally, the actual classifier.\n",
        "    LogisticRegression())\n",
        "\n",
        "# Run cross-validation.\n",
        "# Note that we're using MNE's cross_val_multiscore() here, not scikit-learn's\n",
        "# cross_val_score() as above. We simply pass the number of desired CV splits,\n",
        "# and MNE will automatically do the rest for us.\n",
        "\n",
        "n_splits = 5\n",
        "cv=KFOLD(random_state=32 , n_splits=5)\n",
        "scoring = 'roc_auc'\n",
        "scores = cross_val_multiscore(clf, X, y, cv=cv, scoring='roc_auc')\n",
        "\n",
        "# Mean and standard deviation of ROC AUC across cross-validation runs.\n",
        "roc_auc_mean = round(np.mean(scores), 3)\n",
        "roc_auc_std = round(np.std(scores), 3)\n",
        "\n",
        "print(f'CV scores: {scores}')\n",
        "print(f'Mean ROC AUC = {roc_auc_mean:.3f}' , f'SD = {roc_auc_std:.3f}')"
      ],
      "metadata": {
        "id": "Do1KYxLgSNGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Decoding over time: Comparisons at every single time point.\n",
        "\"\"\"\n",
        "#In the previous examples, we have trained a classifier to discriminate between experimental conditions by using the spatio-temporal patterns of entire trials.\n",
        "#Consequently, the classifier was (hopefully!) able to predict which activation patterns belonged to which condition.\n",
        "#However, an interesting neuroscientific is: Exactly when do the brain signals for two conditions differ?\n",
        "#We can try to answer this question by fitting a classifier at every single time point.\n",
        "#If the classifier can successfully discriminate between the two conditions,\n",
        "#we can conclude that the spatial activation patterns measured by the MEG or EEG sensors differed at this very time point.\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mne.decoding import SlidingEstimator\n",
        "\n",
        "# First, create X and y.\n",
        "epochs_auditory_grad = epochs_auditory.copy().pick_types(meg='grad')\n",
        "X = epochs_auditory_grad.get_data()\n",
        "y = epochs_auditory_grad.events[:, 2]\n",
        "\n",
        "# Classifier pipeline. No need for vectorization as in the previous example.\n",
        "clf = make_pipeline(StandardScaler(),LogisticRegression())\n",
        "\n",
        "# The \"sliding estimator\" will train the classifier at each time point.\n",
        "scoring = 'roc_auc'\n",
        "time_decoder = SlidingEstimator(clf, scoring=scoring, n_jobs=1, verbose=True)\n",
        "\n",
        "# Run cross-validation.\n",
        "n_splits = 5\n",
        "scores = cross_val_multiscore(time_decoder, X, y, cv=5, n_jobs=1)\n",
        "\n",
        "# Mean scores across cross-validation splits, for each time point.\n",
        "mean_scores = np.mean(scores, axis=0)\n",
        "\n",
        "# Mean score across all time points.\n",
        "mean_across_all_times = round(np.mean(scores), 3)\n",
        "print(f'\\n=> Mean CV score across all time points: {mean_across_all_times:.3f}')\n",
        "\n",
        "\n",
        "##Plot the classification results!\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.axhline(0.5, color='k', linestyle='--', label='chance')    # AUC = 0.5\n",
        "ax.axvline(0, color='k', linestyle='-')                       # Mark time point zero.\n",
        "ax.set_xlabel('Time (s)')\n",
        "ax.set_ylabel('Mean ROC AUC')\n",
        "ax.legend()\n",
        "ax.set_title('Left vs Right')\n",
        "fig.suptitle('Sensor Space Decoding')\n",
        "ax.plot(epochs.times, mean_scores, label='score')\n"
      ],
      "metadata": {
        "id": "0xwHBIFzTcq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0qOpslKODB3"
      },
      "outputs": [],
      "source": [
        "##Frequency and time-frequency sensors analysis##\n",
        "\n",
        "#The objective is to show you how to explore the spectral content of your data (frequency and time-frequency).\n",
        "------------------------------------------------------------\n",
        "import pathlib\n",
        "import matplotlib\n",
        "\n",
        "import mne\n",
        "import mne_bids\n",
        "\n",
        "matplotlib.use('Qt5Agg')\n",
        "mne.set_log_level('warning')\n",
        "\n",
        "\n",
        "#Set parameters\n",
        "epochs = mne.read_epochs(pathlib.Path('out_data') / 'epochs_epo.fif')\n",
        "\n",
        "epochs.apply_proj()\n",
        "epochs_auditory = epochs['Auditory']\n",
        "\n",
        "\n",
        "#Frequency analysis\n",
        "#Let's first check out all channel types by averaging across epochs.\n",
        "\n",
        "epochs_auditory.plot_psd(fmin=2., fmax=40., average=True, bandwidth=2)\n",
        "\n",
        "\n",
        "#Select a frequency range in the plot to inspect topographies\n",
        "#The \"bandwidth\" parameter controls the spectral resolution of the multitaper.\n",
        "#You can increase the resolution by chosing a narrower bandwidth at the cost of longer computation time.\n",
        "#Now let's take a look at the spatial distributions of the PSD.\n",
        "\n",
        "epochs_auditory.plot_psd_topomap(ch_type='eeg', normalize=False)\n",
        "epochs_auditory.plot_psd_topomap(ch_type='grad', normalize=True)       ------#different in the visualization a bit with the Normalization!\n",
        "epochs_auditory.plot_psd_topomap(ch_type='mag', normalize=False ,      ------#you can do it with defining the bands!\n",
        "                                 bands=[(0,10 , 'custom1'),\n",
        "                                        (7.5 , 15 , 'custom2')])\n",
        "\n",
        "\n",
        "##Time-frequency analysis: power and inter-trial coherence!!\n",
        "#We now compute time-frequency representations (TFRs) from our Epochs.\n",
        "#We'll look at power and inter-trial coherence (ITC).\n",
        "#To this we'll use the function mne.time_frequency.tfr_morlet but you can also use mne.time_frequency.tfr_multitaper or mne.time_frequency.tfr_stockwell.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# define frequencies of interest (log-spaced)\n",
        "\n",
        "freqs = np.logspace(*np.log10([2, 30]), num=20)\n",
        "freqs\n",
        "\n",
        "n_cycles = freqs / 2.            # different number of cycle per frequency\n",
        "\n",
        "power, itc = mne.time_frequency.tfr_morlet(epochs_auditory, freqs=freqs, n_cycles=n_cycles,\n",
        "                                           use_fft=True,return_itc=True, decim=3, n_jobs=1)\n",
        "\n",
        "power.crop(-0.1, 0.7)            # crop to remove edge artifacts\n",
        "itc.crop(-0.1, 0.7)              # crop to remove edge artifacts\n",
        "\n",
        "\n",
        "\n",
        "###Inspect power\n",
        "#In the topo you can click on an image to visualize the data for one sensor.\n",
        "#You can also select a portion in the time-frequency plane to obtain a topomap for a certain time-frequency region.\n",
        "\n",
        "baseline_mode = 'logratio'\n",
        "baseline = (None, 0)\n",
        "\n",
        "power.copy().pick_types(eeg=True, meg=False).plot_topo()\n",
        "\n",
        "   #Plot power of an individual channel\n",
        "power.plot(picks='EEG 050', baseline=baseline, mode=baseline_mode)\n",
        "  #Plot topomaps for specified frequency ranges\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axis = plt.subplots(1, 3, figsize=(7, 4))\n",
        "power.plot_topomap(ch_type='grad', tmin=0.5, tmax=1.5, fmin=4, fmax=7,\n",
        "                   baseline=baseline, mode=baseline_mode, axes=axis[0],\n",
        "                   title='Theta', show=False, contours=1)\n",
        "power.plot_topomap(ch_type='grad', tmin=0.5, tmax=1.5, fmin=8, fmax=12,\n",
        "                   baseline=baseline, mode=baseline_mode, axes=axis[1],\n",
        "                   title='Alpha', show=False, contours=1)\n",
        "power.plot_topomap(ch_type='grad', tmin=0.5, tmax=1.5, fmin=15, fmax=30,\n",
        "                   baseline=baseline, mode=baseline_mode, axes=axis[2],\n",
        "                   title='Beta', show=False, contours=1)\n",
        "mne.viz.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "####Joint Plot\n",
        "#You can also create a joint plot showing both the aggregated TFR across channels and topomaps at specific times and frequencies\n",
        "#to obtain a quick overview regarding oscillatory effects across time and space.\n",
        "\n",
        "power.plot_joint(baseline=baseline, mode='mean', tmin=None, tmax=None,\n",
        "                 timefreqs=[(0.05, 2.), (0.1, 11.)])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "####Inspect ITC\n",
        "itc.plot_topo(title='Inter-Trial coherence', vmin=0., vmax=0.5, cmap='Reds')\n",
        "\n",
        "##Baseline correction can be applied to power or done in plots.\n",
        "##To illustrate the baseline correction in plots:\n",
        " power.apply_baseline(baseline=(-0.5, 0), mode='logratio')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"SOURCE ESTIMATION----EEG TO MRI!!\"\"\""
      ],
      "metadata": {
        "id": "28c48Y5GT6Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"SOURCE ESTIMATION----EEG TO MRI!!\"\"\"\n",
        "\n",
        "#NOW:\n",
        "####Source estimation\n",
        "\n",
        "%matplotlib qt\n",
        "import pathlib\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import mne_bids\n",
        "import mne\n",
        "\n",
        "matplotlib.use('Qt5Agg')\n",
        "mne.set_log_level('warning')\n",
        "\n",
        "#Start with some fresh epochs\n",
        "bids_root = pathlib.Path('out_data/sample_BIDS')\n",
        "\n",
        "bids_path = mne_bids.BIDSPath(subject='01',\n",
        "                              session='01',\n",
        "                              task='audiovisual',\n",
        "                              run='01',\n",
        "                              datatype='meg',\n",
        "                              root=bids_root)\n",
        "\n",
        "raw = mne_bids.read_raw_bids(bids_path)\n",
        "raw.load_data()\n",
        "raw.filter(l_freq=0.1, h_freq=40)\n",
        "events, event_id = mne.events_from_annotations(raw)\n",
        "\n",
        "tmin = -0.200\n",
        "tmax = 0.500\n",
        "baseline = (None, 0)\n",
        "\n",
        "epochs = mne.Epochs(raw,\n",
        "                    events=events,\n",
        "                    event_id=event_id,\n",
        "                    tmin=tmin,\n",
        "                    tmax=tmax,\n",
        "                    baseline=baseline,\n",
        "                    preload=True,\n",
        "                    proj=False)\n",
        "\n",
        "epochs.save(pathlib.Path('out_data') / 'epochs_for_source_epo.fif')\n",
        "epochs.info\n",
        "\n",
        "\n",
        "###The most important part###\n",
        "##View the BEM\n",
        "subjects_dir = pathlib.Path(mne.datasets.sample.data_path()) / 'subjects'\n",
        "\n",
        "mne.viz.plot_bem(subject='sample', subjects_dir=subjects_dir, orientation='coronal')     #like an MRI gives you the location!!\n",
        "\n",
        "\n",
        "##co-registration:have the positions of sensors of EEG on the MRI pictures:\n",
        "\n",
        "epochs_fname = pathlib.Path('out_data') / 'epochs_for_source_epo.fif'\n",
        "\n",
        "mne.gui.coregistration(subject='sample', subjects_dir=subjects_dir,inst=epochs_fname)\n",
        "\n",
        "\n",
        "trans_fname = pathlib.Path('out_data') / 'sample_new.trans.fif'  --#the path it saves in our pc\n",
        "info = mne.io.read_info(epochs_fname)\n",
        "mne.viz.set_3d_backend('pyvista')\n",
        "\n",
        "fig = mne.viz.plot_alignment(info=info, trans=trans_fname, subject='sample', dig=True,\n",
        "                             subjects_dir=subjects_dir, verbose=True)\n",
        "\n",
        "##Compute the source space\n",
        "subject = 'sample'\n",
        "src = mne.setup_source_space(subject=subject,\n",
        "                             spacing='oct4',  # Use oct6 during an actual analysis!\n",
        "                             subjects_dir=subjects_dir,\n",
        "                             add_dist=False)  # Remove this one during an actual analysis!\n",
        "\n",
        "src\n",
        "\n",
        "\n",
        "mne.viz.plot_alignment(info=info, trans=trans_fname, subject=subject,\n",
        "                       src=src, subjects_dir=subjects_dir, dig=True,\n",
        "                       surfaces=['head-dense', 'white'], coord_frame='meg')\n",
        "\n",
        "##Compute the forward solution!\n",
        "conductivity = (0.3,)               # for single layer – used in MEG\n",
        "conductivity = (0.3, 0.006, 0.3)    # for three layers – used in EEG\n",
        "\n",
        "model = mne.make_bem_model(subject=subject, ico=4,\n",
        "                           conductivity=conductivity,\n",
        "                           subjects_dir=subjects_dir)\n",
        "model\n",
        "\n",
        "bem_sol = mne.make_bem_solution(model)\n",
        "bem_sol\n",
        "\n",
        "bem_fname = pathlib.Path('out_data') / 'sample_bem.fif'\n",
        "mne.bem.write_bem_solution(bem_fname, bem_sol)\n",
        "fwd = mne.make_forward_solution(raw_fname,\n",
        "                                trans=trans_fname,\n",
        "                                src=src,\n",
        "                                bem=bem_sol,\n",
        "                                meg=True, # include MEG channels\n",
        "                                eeg=False, # exclude EEG channels\n",
        "                                mindist=5.0, # ignore sources <= 5mm from inner skull\n",
        "                                n_jobs=1) # number of jobs to run in parallel\n",
        "fwd\n",
        "\n",
        "fwd_fname = pathlib.Path('out_data') / 'sample_fwd.fif'\n",
        "mne.write_forward_solution(fwd_fname, fwd, overwrite=True)\n",
        "\n",
        "\n",
        "##Compute noise covariance\n",
        "noise_cov = mne.compute_covariance(epochs, tmax=0.,\n",
        "                                   method=['shrunk', 'empirical'],\n",
        "                                   rank='info')\n",
        "\n",
        "mne.viz.plot_cov(noise_cov, info=info)\n",
        "\n",
        "epochs_average().plot_white(noise_cov)\n",
        "\n",
        "##Create the inverse operator\n",
        "from mne.forward import read_forward_solution\n",
        "from mne.minimum_norm import (make_inverse_operator, apply_inverse,\n",
        "                              write_inverse_operator)\n",
        "\n",
        "fwd_fname = os.path.join(data_path,\n",
        "    'derivatives/meg_derivatives/sub-01/ses-meg/meg/sub-01-meg-fwd.fif')\n",
        "fwd = mne.read_forward_solution(fwd_fname)\n",
        "fwd = mne.convert_forward_solution(fwd, surf_ori=True)\n",
        "\n",
        "# Restrict forward solution as necessary for MEG\n",
        "fwd = mne.pick_types_forward(fwd, meg=True, eeg=False)\n",
        "\n",
        "# make an M/EEG, MEG-only, and EEG-only inverse operator\n",
        "info = contrast.info\n",
        "inverse_operator = make_inverse_operator(info, fwd, noise_cov,\n",
        "                                         loose=0.2, depth=0.8)\n",
        "\n",
        "#Apply the inverse operator -> Calculate the source estimate\n",
        "method = \"dSPM\"\n",
        "snr = 3.\n",
        "lambda2 = 1. / snr ** 2\n",
        "stc = apply_inverse(epochs['Auditory/Left'], inverse_operator, lambda2,\n",
        "                    method=method, pick_ori=None)\n",
        "\n",
        "brain = stc.plot(surface='inflated',                    #all the brain:inflated , the cortex:pial  ,white matter:white\n",
        "                 hemi='both',\n",
        "                 subjects_dir=subjects_dir,\n",
        "                 time_viewer=True)"
      ],
      "metadata": {
        "id": "5qgehhE4T1Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PLsjlpHuUFWb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}